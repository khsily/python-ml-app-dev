{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 최적의 알고리즘과 매개변수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')  # FutureWarnibngs 경고 제거\n",
    "\n",
    "# 붓꽃 데이터 읽어 들이기\n",
    "iris_data = pd.read_csv(\"data/iris.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# 붓꽃 데이터를 레이블과 입력 데이터로 분리하기\n",
    "y = iris_data[\"Name\"]  # 라벨\n",
    "x = iris_data.drop(\"Name\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 최적의 알고리즘 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float: left;\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>관점</th>\n",
    "            <th>고려할 점</th>\n",
    "            <th>해결 방법</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>알고리즘 선정</td>\n",
    "            <td>더 높은 정답률을 내는 다른 알고리즘이 있지는 않은가?</td>\n",
    "            <td>각각의 알고리즘으로 정답률을 구하고 비교하기</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>알고리즘 평가</td>\n",
    "            <td>데이터(학습 전용과 테스트 전용) 분류의 편향이 발생하지는 않았는가?</td>\n",
    "            <td>크로스 벨리데이션</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 알고리즘의 정답률 비교하기 (붓꽃 분류 프로그램)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier 의 정답률 = 0.9666666666666667\n",
      "BaggingClassifier 의 정답률 = 0.9666666666666667\n",
      "BernoulliNB 의 정답률 = 0.2\n",
      "CalibratedClassifierCV 의 정답률 = 0.9666666666666667\n",
      "ComplementNB 의 정답률 = 0.8\n",
      "DecisionTreeClassifier 의 정답률 = 1.0\n",
      "ExtraTreeClassifier 의 정답률 = 0.9666666666666667\n",
      "ExtraTreesClassifier 의 정답률 = 0.9666666666666667\n",
      "GaussianNB 의 정답률 = 0.9666666666666667\n",
      "GaussianProcessClassifier 의 정답률 = 0.9666666666666667\n",
      "GradientBoostingClassifier 의 정답률 = 0.9666666666666667\n",
      "KNeighborsClassifier 의 정답률 = 1.0\n",
      "LabelPropagation 의 정답률 = 0.9666666666666667\n",
      "LabelSpreading 의 정답률 = 0.9666666666666667\n",
      "LinearDiscriminantAnalysis 의 정답률 = 1.0\n",
      "LinearSVC 의 정답률 = 1.0\n",
      "LogisticRegression 의 정답률 = 1.0\n",
      "LogisticRegressionCV 의 정답률 = 0.9666666666666667\n",
      "MLPClassifier 의 정답률 = 1.0\n",
      "MultinomialNB 의 정답률 = 0.5666666666666667\n",
      "NearestCentroid 의 정답률 = 0.9333333333333333\n",
      "NuSVC 의 정답률 = 0.9333333333333333\n",
      "PassiveAggressiveClassifier 의 정답률 = 0.6\n",
      "Perceptron 의 정답률 = 0.8\n",
      "QuadraticDiscriminantAnalysis 의 정답률 = 0.9666666666666667\n",
      "RadiusNeighborsClassifier 의 정답률 = 0.9333333333333333\n",
      "RandomForestClassifier 의 정답률 = 0.9666666666666667\n",
      "RidgeClassifier 의 정답률 = 0.9666666666666667\n",
      "RidgeClassifierCV 의 정답률 = 0.9666666666666667\n",
      "SGDClassifier 의 정답률 = 0.8\n",
      "SVC 의 정답률 = 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "# 학습 전용과 테스트 전용 분리하기\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, train_size=0.8, shuffle=True)\n",
    "\n",
    "# classifier 알고리즘 모두 추출하기\n",
    "allAlgorithms = all_estimators(type_filter=\"classifier\")  # (알고리즘 이름, 알고리즘 클래스) 형태의 튜플 반환\n",
    "\n",
    "for(name, algorithm) in allAlgorithms:\n",
    "    # 각 알고리즘 객체 생성하기\n",
    "    clf = algorithm()\n",
    "    \n",
    "    # 학습하고 평가하기\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(name, \"의 정답률 =\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 크로스 밸리데이션 (교차 검증)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 프로그램은 각 알고리즘의 정답률을 비교했지만, 평가 횟수가 1회밖에 없고, 사용한 데이터도 한 가지 패턴밖에 없었음.\n",
    "그에따라 학습데이터가 적은 경우 평가의 신뢰성이 떨어질 수 있는데, 이를 해결하기 위해 **'크로스 밸리데이션(cross-validatiobn)'**을 사용\n",
    "\n",
    "#### k-분할 밸리데이션 (3-분할 밸리데이션)\n",
    "- 데이터를 A,B,C라는 3개의 그룹으로 분할한다.\n",
    "- A와 B를 학습 전용 데이터, C를 평가 전용 데이터로 사용해 정답률을 구한다.\n",
    "- B와 C를 학습 전용 데이터, A를 평가 전용 데이터로 사용해 정답률을 구한다.\n",
    "- C와 A를 학습 전용 데이터, B를 평가 전용 데이터로 사용해 정답률을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier 의 정답률 =\n",
      "[0.93333333 0.93333333 0.86666667 0.96666667 0.96666667]\n",
      "BaggingClassifier 의 정답률 =\n",
      "[0.96666667 0.96666667 0.93333333 0.96666667 0.93333333]\n",
      "BernoulliNB 의 정답률 =\n",
      "[0.23333333 0.23333333 0.3        0.3        0.23333333]\n",
      "CalibratedClassifierCV 의 정답률 =\n",
      "[0.93333333 0.93333333 0.9        1.         0.86666667]\n",
      "ComplementNB 의 정답률 =\n",
      "[0.7        0.7        0.56666667 0.66666667 0.7       ]\n",
      "DecisionTreeClassifier 의 정답률 =\n",
      "[0.9        0.93333333 0.96666667 0.93333333 1.        ]\n",
      "ExtraTreeClassifier 의 정답률 =\n",
      "[0.93333333 0.93333333 1.         0.96666667 0.93333333]\n",
      "ExtraTreesClassifier 의 정답률 =\n",
      "[1.         1.         0.86666667 1.         0.9       ]\n",
      "GaussianNB 의 정답률 =\n",
      "[0.93333333 1.         0.96666667 1.         0.86666667]\n",
      "GaussianProcessClassifier 의 정답률 =\n",
      "[0.9        0.96666667 0.9        1.         0.93333333]\n",
      "GradientBoostingClassifier 의 정답률 =\n",
      "[1.         1.         0.9        0.96666667 0.96666667]\n",
      "KNeighborsClassifier 의 정답률 =\n",
      "[0.93333333 0.96666667 1.         0.93333333 1.        ]\n",
      "LabelPropagation 의 정답률 =\n",
      "[1.         0.93333333 0.93333333 1.         0.9       ]\n",
      "LabelSpreading 의 정답률 =\n",
      "[0.93333333 0.96666667 0.93333333 1.         0.93333333]\n",
      "LinearDiscriminantAnalysis 의 정답률 =\n",
      "[1.         1.         0.96666667 0.96666667 0.96666667]\n",
      "LinearSVC 의 정답률 =\n",
      "[0.96666667 0.9        0.96666667 1.         1.        ]\n",
      "LogisticRegression 의 정답률 =\n",
      "[0.93333333 0.96666667 0.96666667 0.9        0.96666667]\n",
      "LogisticRegressionCV 의 정답률 =\n",
      "[1.         0.96666667 0.93333333 0.83333333 0.96666667]\n",
      "MLPClassifier 의 정답률 =\n",
      "[0.96666667 0.9        0.96666667 1.         0.96666667]\n",
      "MultinomialNB 의 정답률 =\n",
      "[0.96666667 0.56666667 0.9        0.5        0.93333333]\n",
      "NearestCentroid 의 정답률 =\n",
      "[0.9        0.96666667 0.9        0.9        0.9       ]\n",
      "NuSVC 의 정답률 =\n",
      "[1.         0.93333333 0.96666667 0.96666667 0.93333333]\n",
      "PassiveAggressiveClassifier 의 정답률 =\n",
      "[0.7        0.6        0.7        0.73333333 0.63333333]\n",
      "Perceptron 의 정답률 =\n",
      "[0.8        0.36666667 0.43333333 0.76666667 0.93333333]\n",
      "QuadraticDiscriminantAnalysis 의 정답률 =\n",
      "[0.96666667 1.         0.9        1.         1.        ]\n",
      "RadiusNeighborsClassifier 의 정답률 =\n",
      "[0.9        0.96666667 1.         0.93333333 0.93333333]\n",
      "RandomForestClassifier 의 정답률 =\n",
      "[0.96666667 0.93333333 0.9        0.93333333 0.96666667]\n",
      "RidgeClassifier 의 정답률 =\n",
      "[0.9        0.9        0.73333333 0.76666667 0.83333333]\n",
      "RidgeClassifierCV 의 정답률 =\n",
      "[0.86666667 0.93333333 0.83333333 0.66666667 0.76666667]\n",
      "SGDClassifier 의 정답률 =\n",
      "[0.96666667 0.7        0.6        0.96666667 0.9       ]\n",
      "SVC 의 정답률 =\n",
      "[1.         1.         0.96666667 0.9        0.93333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# classifier 알고리즘 모두 추출하기\n",
    "allAlgorithms = all_estimators(type_filter=\"classifier\")\n",
    "\n",
    "# k-분할 크로스 밸리데이션 전용 객체\n",
    "# n-split: 데이터 분할 수, shuffle: 데이터 분할시 랜덤하게 분할할지 지정\n",
    "kfold_cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for(name, algorithm) in allAlgorithms:\n",
    "    # 각 알고리즘 객체 생성하기\n",
    "    clf = algorithm()\n",
    "    \n",
    "    # score 메서드를 가진 클래스를 대상으로 하기\n",
    "    if hasattr(clf, \"score\"):\n",
    "        # 크로스 밸리데이션\n",
    "        scores = cross_val_score(clf, x, y, cv=kfold_cv)\n",
    "        print(name, \"의 정답률 =\")\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 최적의 매개변수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 매개 변수 = SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "최종 정답률 = 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "# 학습 전용과 테스트 전용 분리하기\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, train_size=0.8, shuffle=True)\n",
    "\n",
    "# 그리드 서치에서 사용할 매개 변수\n",
    "parameters = [\n",
    "    {\"C\": [1, 10, 100, 1000], \"kernel\": [\"linear\"]},\n",
    "    {\"C\": [1, 10, 100, 1000], \"kernel\": [\"rbf\"], \"gamma\": [0.001, 0.0001]},\n",
    "    {\"C\": [1, 10, 100, 1000], \"kernel\": [\"sigmoid\"], \"gamma\": [0.001, 0.0001]},\n",
    "]\n",
    "\n",
    "# 그리드 서치\n",
    "kfold_cv = KFold(n_splits=5, shuffle=True)\n",
    "clf = GridSearchCV(SVC(), parameters, cv=kfold_cv)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"최적의 매개 변수 =\", clf.best_estimator_)\n",
    "\n",
    "# 최적의 매개 변수로 평가하기\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"최종 정답률 =\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
